[//]: # (# 邮件分类系统技术文档)

[//]: # ()
[//]: # (## 一、多项式朴素贝叶斯分类器实现)

[//]: # ()
[//]: # (### 1.1 核心概率模型)

[//]: # ()
[//]: # (**条件独立性假设**：)

[//]: # (- 假设所有特征（词项）在给定类别条件下相互独立)

[//]: # (- 数学表达：)

[//]: # (  $$)

[//]: # (  P&#40;\mathbf{x}|y&#41; = \prod_{i=1}^n P&#40;x_i|y&#41;)

[//]: # (  $$)

[//]: # (  其中$\mathbf{x} = &#40;x_1,...,x_n&#41;$为特征向量，$y$为类别标签)

[//]: # ()
[//]: # (**多项式分布假设**：)

[//]: # (- 特征服从多项式分布，建模词项出现次数：)

[//]: # (  $$)

[//]: # (  P&#40;\mathbf{x}|y&#41; = \frac{&#40;\sum_i x_i&#41;!}{\prod_i x_i!}\prod_i P&#40;w_i|y&#41;^{x_i})

[//]: # (  $$)

[//]: # ()
[//]: # (### 1.2 贝叶斯分类决策)

[//]: # ()
[//]: # (**分类决策函数**：)

[//]: # ($$)

[//]: # (\hat{y} = \arg\max_{y} P&#40;y&#41;\prod_{i=1}^n P&#40;x_i|y&#41;)

[//]: # ($$)

[//]: # ()
[//]: # (**具体实现步骤**：)

[//]: # ()
[//]: # (1. 先验概率计算：)

[//]: # (   $$)

[//]: # (   P&#40;y&#41; = \frac{N_y + \alpha}{N + \alpha K})

[//]: # (   $$)

[//]: # (   其中$N_y$是类别$y$的样本数，$K$是类别数，$\alpha$为平滑系数)

[//]: # ()
[//]: # (2. 条件概率计算（使用拉普拉斯平滑）：)

[//]: # (   $$)

[//]: # (   P&#40;w_i|y&#41; = \frac{count&#40;w_i,y&#41; + \alpha}{\sum_w count&#40;w,y&#41; + \alpha|V|})

[//]: # (   $$)

[//]: # (   $|V|$为词汇表大小)

[//]: # ()
[//]: # (3. 对数空间计算（避免下溢）：)

[//]: # (   $$)

[//]: # (   \log P&#40;y|\mathbf{x}&#41; \propto \log P&#40;y&#41; + \sum_{i=1}^n x_i \log P&#40;w_i|y&#41;)

[//]: # (   $$)

[//]: # ()
[//]: # (### 1.3 算法优势分析)

[//]: # ()
[//]: # (| 优势 | 说明 |)

[//]: # (|------|------|)

[//]: # (| 计算高效 | 时间复杂度O&#40;nd&#41;，n为样本数，d为特征数 |)

[//]: # (| 小样本友好 | 参数估计只需统计词频 |)

[//]: # (| 高维适应 | 适合文本数据的高维稀疏特性 |)

[//]: # (| 自动特征选择 | 低频词自然获得低权重 |)

[//]: # ()
[//]: # (## 二、特征工程实现对比)

[//]: # ()
[//]: # (### 2.1 高频词特征选择)

[//]: # ()
[//]: # (**数学表达**：)

[//]: # ($$)

[//]: # (\text{Features} = \underset{t\in V}{\text{topk}} \left&#40; \sum_{d\in D} \mathbb{I}&#40;t\in d&#41; \right&#41;)

[//]: # ($$)

[//]: # ()
[//]: # (**实现代码**：)

[//]: # (```python)

[//]: # (vectorizer = CountVectorizer&#40;)

[//]: # (    max_features=1000,)

[//]: # (    tokenizer=jieba.cut,)

[//]: # (    stop_words=stopwords)

[//]: # (&#41;)

[//]: # (```)

[//]: # ()
[//]: # (**特点分析**：)

[//]: # (- 计算复杂度：O&#40;NL&#41;，N为文档数，L为平均文档长度)

[//]: # (- 内存消耗：存储词频向量（稀疏矩阵）)

[//]: # (- 局限性：可能保留高频但无区分度的词)

[//]: # ()
[//]: # (### 2.2 TF-IDF特征加权)

[//]: # ()
[//]: # (**数学定义**：)

[//]: # ($$)

[//]: # (\text{tfidf}&#40;t,d,D&#41; = \underbrace{\frac{f_{t,d}}{\sum_{t'}f_{t',d}}}_{\text{tf}} \times \underbrace{\log\frac{|D|}{|\{d'\in D:t\in d'\}|}}_{\text{idf}})

[//]: # ($$)

[//]: # ()
[//]: # (**实现代码**：)

[//]: # (```python)

[//]: # (vectorizer = TfidfVectorizer&#40;)

[//]: # (    max_features=1000,)

[//]: # (    tokenizer=jieba.cut,)

[//]: # (    stop_words=stopwords,)

[//]: # (    norm='l2')

[//]: # (&#41;)

[//]: # (```)

[//]: # ()
[//]: # (**特性对比**：)

[//]: # ()
[//]: # (| 特性 | 高频词 | TF-IDF |)

[//]: # (|------|--------|--------|)

[//]: # (| 权重类型 | 整数频次 | 连续值 |)

[//]: # (| 常见词处理 | 保留 | 抑制 |)

[//]: # (| 稀有词处理 | 忽略 | 增强 |)

[//]: # (| 计算开销 | 低 | 中等 |)

[//]: # (| 效果 | 基线 | 通常更优 |)

[//]: # ()
[//]: # (## 三、系统运行结果)

[//]: # ()
[//]: # (### 3.1 高频词特征分类效果)

[//]: # (![高频词特征运行结果]&#40;path/to/freq_feat.png&#41;)

[//]: # ()
[//]: # (### 3.2 TF-IDF特征分类效果  )

[//]: # (![TF-IDF运行结果]&#40;path/to/tfidf_feat.png&#41;)

[//]: # ()
[//]: # (### 3.3 性能指标对比)

[//]: # ()
[//]: # (| 指标 | 高频词 | TF-IDF |)

[//]: # (|------|--------|--------|)

[//]: # (| 准确率 | 89.2% | 92.7% |)

[//]: # (| F1-score | 0.876 | 0.913 |)

[//]: # (| 训练时间 | 1.2s | 1.8s |)

[//]: # (| 预测时间 | 0.3s | 0.4s |)

[//]: # ()
[//]: # (## 四、工程实践建议)

[//]: # ()
[//]: # (1. **特征选择优化**：)

[//]: # (   - 结合卡方检验选择最具区分度的特征)

[//]: # (   - 尝试n-gram特征（参数`ngram_range=&#40;1,2&#41;`）)

[//]: # ()
[//]: # (2. **模型改进方向**：)

[//]: # (   ```python)

[//]: # (   from sklearn.naive_bayes import ComplementNB)

[//]: # (   # 对不平衡数据表现更好)

[//]: # (   model = ComplementNB&#40;&#41;)

[//]: # (   ```)

[//]: # ()
[//]: # (3. **生产环境部署**：)

[//]: # (   - 使用`joblib`持久化模型和向量化器)

[//]: # (   - 实现增量学习支持：)

[//]: # (   ```python)

[//]: # (   model.partial_fit&#40;X_batch, y_batch&#41;)

[//]: # (   ```)


# 邮件分类系统技术文档

## 一、多项式朴素贝叶斯分类器实现

### 1.1 核心概率模型

​**​条件独立性假设​**​：
- 假设所有特征（词项）在给定类别条件下相互独立
- 数学表达：
  
  $$P(\mathbf{x}|y) = \prod\_{i=1}^n P(x\_i|y)$$
  
 其中特征向量 $\mathbf{x} = (x\_1,\ ...,\ x\_n)$，$y$ 为类别标签

​**​多项式分布假设​**​：
- 特征服从多项式分布，建模词项出现次数：
  
 $$ P(\mathbf{x}|y) = \frac{ \left( \sum\limits\_{i} x\_i \right)! }{ \prod\limits\_{i} x\_i! } \prod\limits\_{i} \left( P(w\_i|y)^{x\_i} \right) $$

---

### 1.2 贝叶斯分类决策

​**​分类决策函数​**​：
$$\hat{y} = \arg\max\_{y} P(y)\prod\_{i=1}^n P(x\_i|y)$$

​**​具体实现步骤​**​：

1. ​**​先验概率计算​**​：
   
   $$P(y) = \frac{N\_y + \alpha}{N + \alpha K}$$
   
   其中$N\_y$是类别$y$的样本数，$K$是类别数，$\alpha$为平滑系数

2. ​**​条件概率计算（使用拉普拉斯平滑）​**​：
   
   $$P(w\_i|y) = \frac{\text{count}(w\_i,y) + \alpha}{\sum\_{w} \text{count}(w,y) + \alpha|V|}$$
   
   $|V|$为词汇表大小

3. ​**​对数空间计算（避免下溢）​**​：
   
   $$\log P(y|\mathbf{x}) \propto \log P(y) + \sum\_{i=1}^n x\_i \log P(w\_i|y)$$

---

### 1.3 算法优势分析

| 优势                | 说明                                                                 |
|---------------------|----------------------------------------------------------------------|
| ​**​计算高效​**​         | 时间复杂度$O(nd)$，n为样本数，d为特征数                              |
| ​**​小样本友好​**​       | 参数估计只需统计词频                                                  |
| ​**​高维适应​**​         | 适合文本数据的高维稀疏特性                                            |
| ​**​自动特征选择​**​     | 低频词自然获得低权重                                                  |

---

## 二、特征工程实现对比

### 2.1 高频词特征选择

​**​数学表达​**​：
$$
\text{Features} = \text{topk}_{t \in V} \Bigg( \sum_{d \in D} \mathbf{1}_{t \in d} \Bigg)
$$

​**​实现代码​**​：
```python
vectorizer = CountVectorizer(
    max_features=1000,
    tokenizer=jieba.cut,
    stop_words=stopwords
)

### 高频词特征运行结果截图
![高频词特征运行结果](./高频词特征.png)

### TF-IDF运行结果截图
![TF-IDF运行结果](./TF-IDF.png)